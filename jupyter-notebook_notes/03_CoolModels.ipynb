{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(a, b):\n",
    "    assert a.shape == b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(x, y):\n",
    "    perm = np.random.permutation(x.shape[0])\n",
    "    \n",
    "    return x[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation():\n",
    "    '''\n",
    "    Base class for an operation in a neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, input_):\n",
    "        '''\n",
    "        Stores input in the self.input instance variable\n",
    "        Calls the self._output function\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "        self.output = self._output()\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        '''\n",
    "        Calls the self._input_grad() function.\n",
    "        Chekcs that the appropriate shapes match\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self):\n",
    "        '''\n",
    "        The _output method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def _input_grad(self, output_grad):\n",
    "        '''\n",
    "        The _input_grad method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    '''\n",
    "    An Operation with parameters.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        '''\n",
    "        The ParamOperation method\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        '''\n",
    "        Calls self.input_grad and self._param_grad.\n",
    "        Checks appropriate shapes.\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        self.input_grad  = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        '''\n",
    "        Every subclass of ParamOperation must implement _param_grad\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    '''\n",
    "    Weight multiplication operation for a neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, W):\n",
    "        '''\n",
    "        Initialize Operation with self.param = W\n",
    "        '''\n",
    "        super().__init__(W) \n",
    "    \n",
    "    def _output(self):\n",
    "        '''\n",
    "        Compute output\n",
    "        '''\n",
    "        return np.dot(self.input_, self.param)\n",
    "       \n",
    "    def _input_grad(self, output_grad):\n",
    "        '''\n",
    "        Compute input gradient with respect to loss\n",
    "        '''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))    \n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        '''\n",
    "        Compute parameter gradient\n",
    "        '''\n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    Compute bias addition\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, B):\n",
    "        '''\n",
    "        initalize self.param = B\n",
    "        '''\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)        \n",
    "        \n",
    "    def _output(self):\n",
    "        '''\n",
    "        Compute output\n",
    "        '''\n",
    "        return self.input_ + self.param    \n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        '''\n",
    "        Computes input gradient with respect to loss\n",
    "        '''\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        '''\n",
    "        Compute parameter gradient\n",
    "        '''\n",
    "        param_grad = output_grad * np.ones_like(self.param)\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid activation function\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        '''\n",
    "        Compute output\n",
    "        '''\n",
    "        return 1 / (1+np.exp(-1.0 * self.input_))\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        '''\n",
    "        Compute input gradient\n",
    "        '''\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        \n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    'Identity' activation function\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return self.input_\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    '''\n",
    "    A \"layer\" of neurons in a neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params = []\n",
    "        self.param_grads = []\n",
    "        self.operations = []\n",
    "        \n",
    "    def _setup_layer(self, num_in):\n",
    "        '''\n",
    "        The _setup_layer function must be implemented for each layer.\n",
    "        '''\n",
    "        raise NotImplementedEror()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        '''\n",
    "        Passes input forward through a series of operations.\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "            \n",
    "        self.input_ = input_\n",
    "        \n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "            \n",
    "        self.output = input_\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        '''\n",
    "        Passes output_grad backward through a series of operations.\n",
    "        Checks appropriate shapes.\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        \n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "        \n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        '''\n",
    "        Extracts the _param_grads from a layer's operations\n",
    "        '''\n",
    "        \n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "    \n",
    "    def _params(self):\n",
    "        '''\n",
    "        Extracts the _params from a layer's operations\n",
    "        '''\n",
    "        \n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    '''\n",
    "    A fully conneted layer that inherits from \"Layer.\"\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, neurons, activation):\n",
    "        '''\n",
    "        Requires an activation function upon intialization.\n",
    "        this program uses sigmoid as the activation function\n",
    "        '''\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def _setup_layer(self, input_):\n",
    "        '''\n",
    "        Defines the operations of a fully connected layer.\n",
    "        '''\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        self.params = []\n",
    "        \n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "        \n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "        \n",
    "        self.operations = [WeightMultiply(self.params[0]), BiasAdd(self.params[1]), self.activation]\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss():\n",
    "    '''\n",
    "    Objectifies loss of the neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, prediction, target):\n",
    "        '''\n",
    "        computes actual loss\n",
    "        '''\n",
    "        assert_same_shape(prediction, target)\n",
    "        \n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        loss_value = self._output()\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Computes gradients of loss function with respect to its input\n",
    "        '''\n",
    "        self.input_grad = self._input_grad()\n",
    "        \n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "        \n",
    "    \n",
    "    def _output(self):\n",
    "        '''\n",
    "        Every subclass should have its own _output() method\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _input_grad(self):\n",
    "        '''\n",
    "        Every subclass should have its own _input_grad() method\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    '''\n",
    "    The meansquarederror loss of a neural network\n",
    "    '''     \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self):\n",
    "        \n",
    "        loss = np.sum(np.power(self.prediction-self.target, 2)) / self.prediction.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def _input_grad(self):\n",
    "        '''\n",
    "        returns gradient of loss with respect to input for MSE loss'''\n",
    "        \n",
    "        return (2.0 * (self.prediction - self.target) / self.prediction.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, layers, loss, seed):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        \n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)\n",
    "                \n",
    "    def forward(self, x_batch):\n",
    "        '''\n",
    "        Passes data through a series of layers\n",
    "        '''\n",
    "        \n",
    "        x_input = x_batch\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x_input = layer.forward(x_input)\n",
    "        \n",
    "        return x_input\n",
    "    \n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        computes gradient in a backward pass\n",
    "        '''\n",
    "        \n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def train_batch(self, x_batch, y_batch):\n",
    "        '''\n",
    "        passes data forward through layers\n",
    "        returns loss of a layer\n",
    "        passes data backward through the layers\n",
    "        '''\n",
    "        \n",
    "        predictions = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        \n",
    "        # self.loss.backward() function call is passed through to seflf.backward()\n",
    "        # and is the first value for 'grad' ebefore it is looped\n",
    "        self.backward(self.loss.backward())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def param(self):\n",
    "        '''\n",
    "        Gets the parameters for the network\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "            \n",
    "    def param_grad(self):\n",
    "        '''\n",
    "        Gets the parameter gradient for the the network\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    '''\n",
    "    Base class for a neural network optimizer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, init_learning_rate):\n",
    "        # Every optimizer must have an initial learning rate\n",
    "        self.init_learning_rate = init_learning_rate\n",
    "        \n",
    "    def set(self):\n",
    "        '''\n",
    "        Subclass should have its own defined .set() method\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, init_learning_rate):\n",
    "        super().__init__(init_learning_rate)\n",
    "        \n",
    "    def step(self):\n",
    "        '''\n",
    "        For each parameter, adjust in the appropriate direction, with the magnitude\n",
    "        of the adjusment based of the learning rate\n",
    "        '''\n",
    "        for param, param_grad in zip(self.net.param() , self.net.param_grad()):\n",
    "            param -= self.init_learning_rate * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    '''\n",
    "    Trains a neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, net, optim):\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.start = True\n",
    "        \n",
    "        setattr(self.optim, \"net\", self.net)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate_batches(self, x, y, batch_size):\n",
    "        \n",
    "        assert x.shape[0] == y.shape[0]\n",
    "        \n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            \n",
    "            x_batch = x[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "        yield x_batch, y_batch\n",
    "        \n",
    "        \n",
    "    def fit(self,\n",
    "            x_train,\n",
    "            y_train,\n",
    "            x_test,\n",
    "            y_test,\n",
    "            epochs,\n",
    "            eval_every,\n",
    "            seed,\n",
    "            batch_size):\n",
    "        '''\n",
    "        Fits the neural network on the training data for a certain number of epochs\n",
    "        Evaluates the neurl network every 'evel_every' epochs\n",
    "        '''\n",
    "    \n",
    "        np.random.seed(seed)\n",
    "    \n",
    "        if self.start:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "            self.start = False\n",
    "\n",
    "            self.least_loss = 1e9\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            if (epoch+1) % eval_every:\n",
    "\n",
    "                previous_model = deepcopy(self.net)\n",
    "\n",
    "            x_train, y_train = permute_data(x_train, y_train)\n",
    "\n",
    "            batches = self.generate_batches(x_train, y_train, batch_size)\n",
    "\n",
    "            for i, (x_batch, y_batch) in enumerate(batches):\n",
    "                \n",
    "                loss = self.net.train_batch(x_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "            if (epoch+1) % eval_every == 0:\n",
    "\n",
    "                preds = self.net.forward(x_test)\n",
    "                loss = self.net.loss.forward(preds, y_test)\n",
    "\n",
    "                if loss < self.least_loss:\n",
    "                    print(f\"Validation loss after epoch {epoch+1} is {loss:.3f}\")\n",
    "                    self.least_loss = loss\n",
    "                else:\n",
    "                    print(f\"Loss increased after epoch {epoch+1}, final lass was {self.least_loss:.3f}; using model from epoch {epoch+1-eval_every}\")\n",
    "                    self.net = previous_model\n",
    "                    setattr(self.optim, \"net\", self.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regression_model(model, x_test, y_test):\n",
    "    preds = model.forward(x_test)\n",
    "    \n",
    "    mae = np.mean(np.abs(preds - y_test))\n",
    "    rmse = np.sqrt(np.mean(np.power(preds - y_test, 2)))\n",
    "    \n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "    print(f\"Root Mean Square Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = NeuralNetwork(\n",
    "                                layers = [Dense(neurons = 1, activation=Linear())],\n",
    "                                loss = MeanSquaredError(),\n",
    "                                seed=20190501\n",
    "                                )\n",
    "\n",
    "neural_network = NeuralNetwork(\n",
    "                            layers = [\n",
    "                                Dense(neurons = 13, activation = Sigmoid()),\n",
    "                                Dense(neurons = 1, activation = Linear())],\n",
    "                            loss = MeanSquaredError(),\n",
    "                            seed=20190501\n",
    "                                )\n",
    "\n",
    "deep_network = NeuralNetwork(\n",
    "                            layers = [\n",
    "                                Dense(neurons = 13, activation = Sigmoid()),\n",
    "                                Dense(neurons = 13, activation = Sigmoid()),\n",
    "                                Dense(neurons = 1, activation = Linear())],\n",
    "                            loss = MeanSquaredError(),\n",
    "                            seed=20190501\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d_np(a):\n",
    "    assert a.ndim == 1\n",
    "    \n",
    "    return a.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=.3, random_state=80718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after epoch 10 is 404.322\n",
      "Validation loss after epoch 20 is 294.336\n",
      "Validation loss after epoch 30 is 230.459\n",
      "Validation loss after epoch 40 is 163.284\n",
      "Validation loss after epoch 50 is 103.427\n",
      "\n",
      "Mean Absolute Error: 8.19\n",
      "Root Mean Square Error: 10.17\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(linear_regression, SGD(init_learning_rate=0.01))\n",
    "\n",
    "trainer.fit(x_train, y_train, x_test, y_test,\n",
    "           epochs = 50,\n",
    "           eval_every = 10,\n",
    "           batch_size = 32,\n",
    "           seed = 20190501)\n",
    "print()\n",
    "eval_regression_model(linear_regression, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after epoch 10 is 129.548\n",
      "Validation loss after epoch 20 is 61.646\n",
      "Validation loss after epoch 30 is 57.538\n",
      "Validation loss after epoch 40 is 50.866\n",
      "Validation loss after epoch 50 is 43.314\n",
      "\n",
      "Mean Absolute Error: 4.53\n",
      "Root Mean Square Error: 6.58\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(neural_network, SGD(init_learning_rate = 0.01))\n",
    "\n",
    "trainer.fit(x_train, y_train, x_test, y_test,\n",
    "           epochs = 50,\n",
    "           eval_every = 10,\n",
    "           batch_size = 32,\n",
    "           seed = 20190501)\n",
    "print()\n",
    "eval_regression_model(neural_network, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after epoch 10 is 109.131\n",
      "Validation loss after epoch 20 is 83.087\n",
      "Validation loss after epoch 30 is 78.645\n",
      "Validation loss after epoch 40 is 74.414\n",
      "Validation loss after epoch 50 is 71.426\n",
      "\n",
      "Mean Absolute Error: 6.67\n",
      "Root Mean Square Error: 8.45\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(deep_network, SGD(init_learning_rate=0.01))\n",
    "\n",
    "trainer.fit(x_train, y_train, x_test, y_test,\n",
    "           epochs = 50,\n",
    "           eval_every = 10,\n",
    "           batch_size = 32,\n",
    "           seed = 20190501)\n",
    "print()\n",
    "eval_regression_model(deep_network, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
